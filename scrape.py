import os, json, base64, asyncio
import sys
from io import StringIO
from typing import List, Tuple, Optional, Dict, Any
from datetime import datetime, timezone
import pandas as pd
import hashlib
import requests

from playwright.async_api import async_playwright
import logging

# ---- Google Sheets (gspread) ----
import gspread
from google.oauth2.service_account import Credentials

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# --------- Config ----------
BASE = "https://jobm.edoclite.com/jobManagement"
LOGIN = f"{BASE}/pages/login"
INDEX = f"{BASE}/pages/index"
TABS: List[int] = [13, 14, 15, 8, 7, 11]

# Tab names mapping
TAB_NAMES = {
    13: "InProgress_Jobs",
    14: "Pending_Jobs", 
    15: "Completed_Jobs",
    8: "Urgent_Jobs",
    7: "Review_Jobs",
    11: "Archive_Jobs"
}

USER = os.getenv("EDOCLITE_USER", "").strip()
PASS = os.getenv("EDOCLITE_PASS", "").strip()

SHEET_ID = os.getenv("GOOGLE_SHEET_ID", "").strip()
SVC_JSON_RAW = os.getenv("GOOGLE_SERVICE_ACCOUNT_JSON", "").strip()
SVC_JSON_B64 = os.getenv("GOOGLE_SERVICE_ACCOUNT_JSON_B64", "").strip()

# LINE Notify Config
LINE_NOTIFY_TOKEN = os.getenv("LINE_NOTIFY_TOKEN", "").strip()

# AI Config
EMBEDDING_API_URL = os.getenv("EMBEDDING_API_URL", "http://209.15.123.47:11434/api/embeddings")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "nomic-embed-text:latest")
CHAT_API_URL = os.getenv("CHAT_API_URL", "http://209.15.123.47:11434/api/generate")
CHAT_MODEL = os.getenv("CHAT_MODEL", "Qwen3:14b")

SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive",
]

# Master sheet configuration
MASTER_SHEET_NAME = "Master_Data"
SUMMARY_SHEET_NAME = "Data_Summary"
LOG_SHEET_NAME = "Sync_Logs"

# --------- LINE Notify helpers ----------
def send_line_notify(message: str, token: str = None) -> bool:
    """‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ú‡πà‡∏≤‡∏ô LINE Notify"""
    if not token and not LINE_NOTIFY_TOKEN:
        logger.warning("‚ö†Ô∏è  ‡πÑ‡∏°‡πà‡∏û‡∏ö LINE_NOTIFY_TOKEN")
        return False
    
    try:
        token = token or LINE_NOTIFY_TOKEN
        url = "https://notify-api.line.me/api/notify"
        headers = {"Authorization": f"Bearer {token}"}
        data = {"message": message}
        
        response = requests.post(url, headers=headers, data=data, timeout=10)
        
        if response.status_code == 200:
            logger.info("üì± ‡∏™‡πà‡∏á LINE Notify ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
            return True
        else:
            logger.error(f"‚ùå LINE Notify ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {response.status_code}")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå LINE Notify error: {e}")
        return False

# --------- AI helpers ----------
def get_text_embedding(text: str) -> Optional[List[float]]:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á text embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
    try:
        payload = {
            "model": EMBEDDING_MODEL,
            "prompt": text
        }
        response = requests.post(EMBEDDING_API_URL, json=payload, timeout=30)
        if response.status_code == 200:
            return response.json().get("embedding")
    except Exception as e:
        logger.error(f"‚ùå Embedding API error: {e}")
    return None

def analyze_job_changes(old_data: Dict, new_data: Dict, job_no: str) -> str:
    """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á Job ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ AI"""
    try:
        changes = []
        for key in set(old_data.keys()) | set(new_data.keys()):
            old_val = old_data.get(key, "N/A")
            new_val = new_data.get(key, "N/A")
            if old_val != new_val:
                changes.append(f"{key}: {old_val} ‚Üí {new_val}")
        
        if not changes:
            return "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á"
        
        prompt = f"""
        ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á Job No: {job_no}
        ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á:
        {chr(10).join(changes)}
        
        ‡πÇ‡∏õ‡∏£‡∏î‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏•‡∏∞‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô (‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢):
        """
        
        payload = {
            "model": CHAT_MODEL,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.3,
                "top_p": 0.8
            }
        }
        
        response = requests.post(CHAT_API_URL, json=payload, timeout=60)
        if response.status_code == 200:
            result = response.json()
            return result.get("response", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏î‡πâ")
    except Exception as e:
        logger.error(f"‚ùå AI Analysis error: {e}")
    
    return "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏î‡πâ"

# --------- Google Sheets helpers ----------
def get_gspread_client() -> gspread.Client:
    """‡∏£‡∏±‡∏ö Service Account ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á gspread client"""
    info = None
    
    try:
        if SVC_JSON_B64:
            logger.info("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏ä‡πâ Service Account ‡∏à‡∏≤‡∏Å Base64...")
            decoded_bytes = base64.b64decode(SVC_JSON_B64)
            decoded_str = decoded_bytes.decode("utf-8")
            info = json.loads(decoded_str)
        elif SVC_JSON_RAW:
            logger.info("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏ä‡πâ Service Account ‡∏à‡∏≤‡∏Å JSON ‡∏î‡∏¥‡∏ö...")
            info = json.loads(SVC_JSON_RAW)
        elif os.path.exists("service_account.json"):
            logger.info("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏ä‡πâ Service Account ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå service_account.json...")
            with open("service_account.json", "r", encoding="utf-8") as f:
                info = json.load(f)

        if not info:
            raise RuntimeError("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö Google Service Account JSON")

        required_fields = ['type', 'project_id', 'private_key', 'client_email']
        for field in required_fields:
            if field not in info:
                raise RuntimeError(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ü‡∏¥‡∏•‡∏î‡πå {field} ‡πÉ‡∏ô Service Account JSON")

        creds = Credentials.from_service_account_info(info, scopes=SCOPES)
        client = gspread.authorize(creds)
        logger.info("‚úÖ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Google Sheets API ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        return client
        
    except Exception as e:
        logger.error(f"‚ùå Google Sheets connection error: {e}")
        raise RuntimeError(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Google Sheets: {e}")

def get_or_create_worksheet(sh: gspread.Spreadsheet, title: str, headers: List[str] = None) -> gspread.Worksheet:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏î‡∏∂‡∏á worksheet ‡∏°‡∏≤ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ header"""
    try:
        ws = sh.worksheet(title)
    except gspread.exceptions.WorksheetNotFound:
        logger.info(f"üìÑ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏µ‡∏ï‡πÉ‡∏´‡∏°‡πà '{title}'...")
        ws = sh.add_worksheet(title=title, rows=1000, cols=20)
        
        if headers:
            ws.update("A1", [headers], value_input_option="RAW")
            logger.info(f"‚úÖ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ headers ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ä‡∏µ‡∏ï '{title}'")
    
    return ws

def get_existing_data(ws: gspread.Worksheet) -> Dict[str, Dict]:
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô worksheet ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏≤‡∏° Job No."""
    try:
        records = ws.get_all_records()
        job_data = {}
        
        for record in records:
            job_no = record.get('Job_No', '').strip()
            if job_no:
                job_data[job_no] = record
        
        logger.info(f"üìä ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà: {len(job_data)} jobs")
        return job_data
    except Exception as e:
        logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà: {e}")
        return {}

def create_data_hash(data: Dict) -> str:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á hash ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á"""
    # ‡πÄ‡∏≠‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤ hash (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° timestamp)
    relevant_data = {k: v for k, v in data.items() 
                    if k not in ['Source_Tab', 'Last_Updated', 'Data_Hash']}
    data_str = json.dumps(relevant_data, sort_keys=True, ensure_ascii=False)
    return hashlib.md5(data_str.encode('utf-8')).hexdigest()

def update_master_data(sh: gspread.Spreadsheet, tab_data: Dict[int, pd.DataFrame]) -> Tuple[int, int, int]:
    """‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô Master sheet ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥"""
    ws = get_or_create_worksheet(sh, MASTER_SHEET_NAME, [
        'Job_No', 'Source_Tab', 'Tab_Name', 'Last_Updated', 'First_Seen', 
        'Update_Count', 'Data_Hash', 'Status', 'Priority', 'Description',
        'AI_Summary', 'Change_Log'
    ])
    
    existing_data = get_existing_data(ws)
    current_time = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')
    
    new_jobs = 0
    updated_jobs = 0
    unchanged_jobs = 0
    
    all_records = []
    notifications = []
    
    for tab_num, df in tab_data.items():
        if df.empty:
            continue
            
        tab_name = TAB_NAMES.get(tab_num, f"Tab_{tab_num}")
        
        for _, row in df.iterrows():
            # ‡∏´‡∏≤ Job No. column (‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô)
            job_no = None
            for col in df.columns:
                if 'job' in col.lower() and ('no' in col.lower() or 'number' in col.lower()):
                    job_no = str(row[col]).strip()
                    break
            
            if not job_no or job_no == 'nan' or job_no == '':
                continue
            
            # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
            new_record = {
                'Job_No': job_no,
                'Source_Tab': tab_num,
                'Tab_Name': tab_name,
                'Last_Updated': current_time,
            }
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å DataFrame
            for col, val in row.items():
                if col != 'Job_No':  # ‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥
                    new_record[col] = str(val) if pd.notna(val) else ''
            
            data_hash = create_data_hash(new_record)
            new_record['Data_Hash'] = data_hash
            
            if job_no in existing_data:
                # Job ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
                old_record = existing_data[job_no]
                old_hash = old_record.get('Data_Hash', '')
                
                if old_hash != data_hash:
                    # ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á
                    new_record['First_Seen'] = old_record.get('First_Seen', current_time)
                    new_record['Update_Count'] = int(old_record.get('Update_Count', 0)) + 1
                    
                    # AI Analysis
                    ai_analysis = analyze_job_changes(old_record, new_record, job_no)
                    new_record['AI_Summary'] = ai_analysis
                    new_record['Change_Log'] = f"{old_record.get('Change_Log', '')} | {current_time}: Updated from {tab_name}"
                    
                    updated_jobs += 1
                    notifications.append(f"üîÑ Job {job_no} updated in {tab_name}\n{ai_analysis}")
                    logger.info(f"üîÑ Job {job_no} updated")
                else:
                    # ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á ‡πÅ‡∏ï‡πà‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï timestamp
                    new_record.update(old_record)
                    new_record['Last_Updated'] = current_time
                    unchanged_jobs += 1
            else:
                # Job ‡πÉ‡∏´‡∏°‡πà
                new_record['First_Seen'] = current_time
                new_record['Update_Count'] = 1
                new_record['AI_Summary'] = f"New job detected in {tab_name}"
                new_record['Change_Log'] = f"{current_time}: First seen in {tab_name}"
                
                new_jobs += 1
                notifications.append(f"üÜï New Job {job_no} found in {tab_name}")
                logger.info(f"üÜï New Job {job_no} found")
            
            all_records.append(new_record)
    
    # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô sheet
    if all_records:
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° headers
        all_keys = set()
        for record in all_records:
            all_keys.update(record.keys())
        headers = sorted(all_keys)
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        values = [headers]
        for record in all_records:
            row = [record.get(h, '') for h in headers]
            values.append(row)
        
        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï sheet
        ws.clear()
        ws.resize(rows=len(values), cols=len(headers))
        ws.update("A1", values, value_input_option="RAW")
        
        logger.info(f"‚úÖ ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Master sheet: {len(all_records)} jobs")
    
    # ‡∏™‡πà‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô
    if notifications and LINE_NOTIFY_TOKEN:
        summary = f"üìä Job Data Update Summary:\nüÜï New: {new_jobs}\nüîÑ Updated: {updated_jobs}\n‚è∏Ô∏è Unchanged: {unchanged_jobs}"
        send_line_notify(summary)
        
        # ‡∏™‡πà‡∏á‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡πÅ‡∏¢‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö jobs ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        important_notifications = [n for n in notifications if "üÜï" in n or "üîÑ" in n][:5]
        for notification in important_notifications:
            send_line_notify(notification)
    
    return new_jobs, updated_jobs, unchanged_jobs

def update_summary_sheet(sh: gspread.Spreadsheet, stats: Dict[str, Any]) -> None:
    """‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï summary sheet ‡∏î‡πâ‡∏ß‡∏¢‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô"""
    ws = get_or_create_worksheet(sh, SUMMARY_SHEET_NAME, [
        'Timestamp', 'Total_Jobs', 'New_Jobs', 'Updated_Jobs', 'Unchanged_Jobs',
        'Successful_Tabs', 'Failed_Tabs', 'Processing_Time', 'Status'
    ])
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ñ‡∏ß‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô
    current_time = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')
    new_row = [
        current_time,
        stats.get('total_jobs', 0),
        stats.get('new_jobs', 0),
        stats.get('updated_jobs', 0),
        stats.get('unchanged_jobs', 0),
        ','.join(map(str, stats.get('successful_tabs', []))),
        ','.join(map(str, stats.get('failed_tabs', []))),
        f"{stats.get('processing_time', 0):.2f}s",
        stats.get('status', 'Unknown')
    ]
    
    ws.insert_row(new_row, 2)  # ‡πÅ‡∏ó‡∏£‡∏Å‡∏ó‡∏µ‡πà‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà 2 (‡∏´‡∏•‡∏±‡∏á header)
    logger.info("‚úÖ ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Summary sheet")

def log_sync_activity(sh: gspread.Spreadsheet, activity: str, details: str = "") -> None:
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å log ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô"""
    try:
        ws = get_or_create_worksheet(sh, LOG_SHEET_NAME, [
            'Timestamp', 'Activity', 'Details', 'Status'
        ])
        
        current_time = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')
        ws.insert_row([current_time, activity, details, 'Success'], 2)
    except Exception as e:
        logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å log: {e}")

def upsert_worksheet(sh: gspread.Spreadsheet, title: str, df: pd.DataFrame) -> None:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï worksheet ‡πÉ‡∏ô Google Sheets (‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ)"""
    title = str(title)[:99] if title else "Sheet"
    
    try:
        ws = get_or_create_worksheet(sh, title)
        ws.clear()
        
        if df is None or df.empty:
            ws.update("A1", [["NO DATA"]])
            logger.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ä‡∏µ‡∏ï '{title}'")
            return

        df = df.copy()
        df.columns = [str(c) for c in df.columns]
        values = [list(df.columns)] + df.astype(object).where(pd.notna(df), "").values.tolist()

        rows, cols = len(values), max(len(r) for r in values) if values else 1
        ws.resize(rows=rows, cols=cols)
        ws.update("A1", values, value_input_option="RAW")
        
        logger.info(f"‚úÖ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(df)} ‡πÅ‡∏ñ‡∏ß, {len(df.columns)} ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå")
        
    except Exception as e:
        logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ä‡∏µ‡∏ï '{title}': {e}")
        raise

# --------- Scrape helpers ----------
async def login_with_ui(context) -> Tuple[bool, "Page"]:
    """‡∏•‡πá‡∏≠‡∏Å‡∏≠‡∏¥‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏∞‡∏ö‡∏ö"""
    page = await context.new_page()
    
    try:
        logger.info("üåê ‡πÄ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤‡∏•‡πá‡∏≠‡∏Å‡∏≠‡∏¥‡∏ô...")
        await page.goto(LOGIN, wait_until="domcontentloaded", timeout=30_000)

        logger.info("üë§ ‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πá‡∏≠‡∏Å‡∏≠‡∏¥‡∏ô...")
        await page.fill('input[name="username"]', USER, timeout=10_000)
        await page.fill('input[name="password"]', PASS, timeout=10_000)

        logger.info("üîë ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏•‡πá‡∏≠‡∏Å‡∏≠‡∏¥‡∏ô...")
        await page.click('button[name="login__username"], input[name="login__username"]')

        await page.wait_for_load_state("networkidle", timeout=30_000)
        await page.goto(INDEX, wait_until="domcontentloaded", timeout=30_000)

        current_url = page.url.lower()
        ok = ("login" not in current_url)
        
        if ok:
            logger.info(f"‚úÖ ‡∏•‡πá‡∏≠‡∏Å‡∏≠‡∏¥‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {page.url}")
        else:
            logger.error(f"‚ùå ‡∏•‡πá‡∏≠‡∏Å‡∏≠‡∏¥‡∏ô‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {page.url}")
            
        return ok, page
        
    except Exception as e:
        logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡πá‡∏≠‡∏Å‡∏≠‡∏¥‡∏ô: {e}")
        return False, page

async def extract_tables_from_dom(page, tab: int) -> pd.DataFrame:
    """‡πÄ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏ó‡πá‡∏ö‡πÅ‡∏•‡πâ‡∏ß‡∏î‡∏∂‡∏á‡∏ó‡∏∏‡∏Å <table> ‡∏ö‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô DataFrame ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß"""
    try:
        url = f"{INDEX}?tab={tab}"
        logger.info(f"üåê ‡πÄ‡∏õ‡∏¥‡∏î {url}")
        await page.goto(url, wait_until="domcontentloaded", timeout=30_000)

        await page.wait_for_timeout(2000)

        # ‡∏õ‡∏£‡∏±‡∏ö DataTables page length
        length_sel = page.locator('select[name$="_length"], select.dt-input')
        if await length_sel.count() > 0:
            try:
                logger.info("‚öôÔ∏è ‡∏õ‡∏£‡∏±‡∏ö DataTables page length...")
                opts = await length_sel.first.evaluate("(el)=>Array.from(el.options).map(o=>o.value)")
                for v in ["-1", "1000", "500", "250", "100"]:
                    if v in opts:
                        await length_sel.first.select_option(v)
                        await page.wait_for_load_state("networkidle", timeout=10_000)
                        logger.info(f"‚úÖ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÅ‡∏™‡∏î‡∏á {v} ‡πÅ‡∏ñ‡∏ß‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤")
                        break
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏±‡∏ö page length: {e}")

        # ‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å <table> ‡πÄ‡∏õ‡πá‡∏ô DataFrame ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        tables = await page.locator("table").all()
        logger.info(f"üîç ‡∏û‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á {len(tables)} ‡∏ï‡∏≤‡∏£‡∏≤‡∏á")
        
        frames = []
        for i, t in enumerate(tables):
            try:
                html = await t.evaluate("(el)=>el.outerHTML")
                dfs = pd.read_html(StringIO(html))
                for df in dfs:
                    if not df.empty:
                        df.columns = [str(c).strip() for c in df.columns]
                        frames.append(df)
                        logger.info(f"   üìä ‡∏ï‡∏≤‡∏£‡∏≤‡∏á {i+1}: {len(df)} ‡πÅ‡∏ñ‡∏ß, {len(df.columns)} ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå")
            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á {i+1}: {e}")

        if frames:
            result = pd.concat(frames, ignore_index=True)
            logger.info(f"‚úÖ ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(result)} ‡πÅ‡∏ñ‡∏ß, {len(result.columns)} ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå")
            return result
        else:
            logger.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
            return pd.DataFrame()

    except Exception as e:
        logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ó‡πá‡∏ö {tab}: {e}")
        return pd.DataFrame()

# --------- Main ----------
async def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å"""
    start_time = datetime.now()
    logger.info("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏° Enhanced Web Scraper")
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö environment variables
    if not USER or not PASS:
        logger.error("‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ EDOCLITE_USER ‡πÅ‡∏•‡∏∞ EDOCLITE_PASS")
        raise RuntimeError("Missing login credentials")

    if not SHEET_ID:
        logger.error("‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ GOOGLE_SHEET_ID")
        raise RuntimeError("Missing Google Sheet ID")

    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° client ‡∏Ç‡∏≠‡∏á Google Sheets
    try:
        logger.info("üîó ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Google Sheets...")
        gc = get_gspread_client()
        sh = gc.open_by_key(SHEET_ID)
        logger.info(f"‚úÖ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Google Sheets ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: '{sh.title}'")
        
        log_sync_activity(sh, "Start", "‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ã‡∏¥‡∏á‡∏Ñ‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
    except Exception as e:
        logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Google Sheets: {e}")
        raise

    # ‡πÄ‡∏£‡∏¥‡πà‡∏° Playwright
    try:
        async with async_playwright() as p:
            logger.info("üåê ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Browser...")
            
            launch_options = {
                "headless": True,
                "args": [
                    "--no-sandbox",
                    "--disable-dev-shm-usage", 
                    "--disable-gpu",
                    "--disable-web-security",
                    "--disable-features=VizDisplayCompositor",
                    "--disable-extensions",
                    "--disable-plugins",
                    "--disable-images",
                    "--no-first-run",
                    "--disable-default-apps",
                    "--disable-background-timer-throttling",
                    "--disable-renderer-backgrounding",
                    "--disable-backgrounding-occluded-windows"
                ]
            }
            
            browser = await p.chromium.launch(**launch_options)
            context = await browser.new_context(
                viewport={"width": 1400, "height": 2000},
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
                timezone_id="Asia/Bangkok",
                locale="th-TH",
                ignore_https_errors=True,
            )

            # ‡∏•‡πá‡∏≠‡∏Å‡∏≠‡∏¥‡∏ô
            logger.info("üîê ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏•‡πá‡∏≠‡∏Å‡∏≠‡∏¥‡∏ô...")
            ok, page = await login_with_ui(context)
            if not ok:
                logger.error("‚ùå ‡∏•‡πá‡∏≠‡∏Å‡∏≠‡∏¥‡∏ô‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
                if LINE_NOTIFY_TOKEN:
                    send_line_notify("‚ùå ‡∏•‡πá‡∏≠‡∏Å‡∏≠‡∏¥‡∏ô‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à - ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö username/password")
                await browser.close()
                raise SystemExit(1)

            logger.info("‚úÖ ‡∏•‡πá‡∏≠‡∏Å‡∏≠‡∏¥‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")

            # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡πÅ‡∏ó‡πá‡∏ö
            successful_tabs = []
            failed_tabs = []
            tab_data = {}
            
            for tab_num in TABS:
                try:
                    logger.info(f"‚û°Ô∏è ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ó‡πá‡∏ö {tab_num} ({TAB_NAMES.get(tab_num, f'Tab_{tab_num}')})")
                    df = await extract_tables_from_dom(page, tab_num)
                    
                    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ó‡πá‡∏ö‡πÅ‡∏¢‡∏Å (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ‡πÄ‡∏î‡∏¥‡∏°)
                    sheet_title = f"Tab{tab_num}_{TAB_NAMES.get(tab_num, 'Unknown')}"
                    upsert_worksheet(sh, sheet_title, df)
                    
                    # ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö master data
                    tab_data[tab_num] = df
                    
                    rows_count = len(df) if not df.empty else 0
                    logger.info(f"‚úÖ ‡πÅ‡∏ó‡πá‡∏ö {tab_num} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ({rows_count} ‡πÅ‡∏ñ‡∏ß)")
                    successful_tabs.append(tab_num)
                    
                    await asyncio.sleep(1)
                    
                except Exception as e:
                    logger.error(f"‚ùå ‡πÅ‡∏ó‡πá‡∏ö {tab_num} ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")
                    failed_tabs.append(tab_num)
                    log_sync_activity(sh, "Tab Error", f"‡πÅ‡∏ó‡πá‡∏ö {tab_num} ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {str(e)}")

            await browser.close()
            
            # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Master Data
            logger.info("üìä ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Master Data...")
            new_jobs, updated_jobs, unchanged_jobs = update_master_data(sh, tab_data)
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
            stats = {
                'total_jobs': new_jobs + updated_jobs + unchanged_jobs,
                'new_jobs': new_jobs,
                'updated_jobs': updated_jobs,
                'unchanged_jobs': unchanged_jobs,
                'successful_tabs': successful_tabs,
                'failed_tabs': failed_tabs,
                'processing_time': processing_time,
                'status': 'Success' if not failed_tabs else 'Partial Success'
            }
            
            # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Summary Sheet
            update_summary_sheet(sh, stats)
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å log
            log_details = f"Jobs: {new_jobs} new, {updated_jobs} updated, {unchanged_jobs} unchanged. Tabs: {len(successful_tabs)} success, {len(failed_tabs)} failed"
            log_sync_activity(sh, "Sync Complete", log_details)
            
            # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á LINE Notify
            logger.info("üéâ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô")
            logger.info(f"üìä ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥:")
            logger.info(f"   üÜï Jobs ‡πÉ‡∏´‡∏°‡πà: {new_jobs}")
            logger.info(f"   üîÑ Jobs ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï: {updated_jobs}")
            logger.info(f"   ‚è∏Ô∏è  Jobs ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á: {unchanged_jobs}")
            logger.info(f"   ‚úÖ ‡πÅ‡∏ó‡πá‡∏ö‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(successful_tabs)} {successful_tabs}")
            if failed_tabs:
                logger.warning(f"   ‚ùå ‡πÅ‡∏ó‡πá‡∏ö‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {len(failed_tabs)} {failed_tabs}")
            logger.info(f"   ‚è±Ô∏è  ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: {processing_time:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ")
            
            # ‡∏™‡πà‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏ó‡∏≤‡∏á LINE
            if LINE_NOTIFY_TOKEN:
                summary_msg = f"""
üéâ Job Sync Complete!
üìä ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥:
üÜï Jobs ‡πÉ‡∏´‡∏°‡πà: {new_jobs}
üîÑ Jobs ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï: {updated_jobs} 
‚è∏Ô∏è Jobs ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á: {unchanged_jobs}
‚úÖ ‡πÅ‡∏ó‡πá‡∏ö‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(successful_tabs)}/{len(TABS)}
‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: {processing_time:.1f}s

üìã Sheet: {sh.title}
üîó Link: https://docs.google.com/spreadsheets/d/{SHEET_ID}
                """.strip()
                send_line_notify(summary_msg)
            
            return len(failed_tabs) == 0
            
    except Exception as e:
        processing_time = (datetime.now() - start_time).total_seconds()
        logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á: {e}")
        
        # ‡∏™‡πà‡∏á‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î
        if LINE_NOTIFY_TOKEN:
            error_msg = f"‚ùå Job Sync Failed!\nError: {str(e)}\nTime: {processing_time:.1f}s"
            send_line_notify(error_msg)
        
        try:
            log_sync_activity(sh, "Error", f"‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}")
        except:
            pass
        
        raise

if __name__ == "__main__":
    try:
        success = asyncio.run(main())
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏´‡∏¢‡∏∏‡∏î‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°")
        if LINE_NOTIFY_TOKEN:
            send_line_notify("‚èπÔ∏è Job Sync ‡∏ñ‡∏π‡∏Å‡∏´‡∏¢‡∏∏‡∏î‡πÇ‡∏î‡∏¢‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ")
        sys.exit(1)
    except Exception as e:
        logger.error(f"üí• ‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
        if LINE_NOTIFY_TOKEN:
            send_line_notify(f"üí• Job Sync Crashed: {str(e)}")
        sys.exit(1)
